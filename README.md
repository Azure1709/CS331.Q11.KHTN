# So sÃ¡nh Softmax vÃ  Sigmoid Loss trong Contrastive Learning cho áº¢nh Y táº¿

Äá»“ Ã¡n mÃ´n há»c CS331 - Thá»‹ giÃ¡c mÃ¡y tÃ­nh nÃ¢ng cao.
TrÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ ThÃ´ng tin - ÄHQG-HCM.

## ğŸ‘¥ NhÃ³m thá»±c hiá»‡n
* **Há»“ Ngá»c Luáº­t** - 23520900
* **BÃ¹i Ngá»c ThiÃªn Thanh** - 23521436

## ğŸ“ Giá»›i thiá»‡u
Dá»± Ã¡n nÃ y táº­p trung vÃ o viá»‡c tiá»n huáº¥n luyá»‡n (pretraining) cÃ¡c mÃ´ hÃ¬nh há»c sÃ¢u trÃªn dá»¯ liá»‡u y táº¿ khan hiáº¿m nhÃ£n báº±ng phÆ°Æ¡ng phÃ¡p **conVIRT** (Contrastive learning of medical Visual Representations from Text).

Má»¥c tiÃªu chÃ­nh lÃ  **so sÃ¡nh hiá»‡u quáº£ giá»¯a Softmax Loss vÃ  Sigmoid Loss**, Ä‘á»“ng thá»i Ä‘á» xuáº¥t giáº£i phÃ¡p kháº¯c phá»¥c hiá»‡n tÆ°á»£ng "loss collapse" cá»§a Sigmoid Loss Ä‘á»ƒ cáº£i thiá»‡n biá»ƒu diá»…n hÃ¬nh áº£nh y khoa.

## ğŸ“Š Dá»¯ liá»‡u (Datasets)

1.  **Pretraining:** [MIMIC-CXR](https://physionet.org/content/mimic-cxr/)
    * 257,873 cáº·p áº£nh X-quang vÃ  bÃ¡o cÃ¡o bá»‡nh lÃ½.
    * Tiá»n xá»­ lÃ½: Cáº¯t ngáº«u nhiÃªn cÃ¢u trong bÃ¡o cÃ¡o, biáº¿n Ä‘á»•i áº£nh (Resize, Crop, Flip).

2.  **Downstream Task:** [RSNA Pneumonia Detection](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)
    * BÃ i toÃ¡n: PhÃ¢n loáº¡i nhá»‹ phÃ¢n (CÃ³/KhÃ´ng viÃªm phá»•i).
    * Sá»‘ lÆ°á»£ng: 26,684 áº£nh (77.47% Negative, 22.53% Positive).

## ğŸ“ˆ Káº¿t quáº£ (Results)

ÄÃ¡nh giÃ¡ trÃªn táº­p dá»¯ liá»‡u RSNA vá»›i cÃ¡c tá»· lá»‡ dá»¯ liá»‡u huáº¥n luyá»‡n khÃ¡c nhau (1%, 10%, 100%).

| Method | 1% Data (F1/AUC) | 10% Data (F1/AUC) | 100% Data (F1/AUC) |
| :--- | :---: | :---: | :---: |
| Resnet50 ImageNet | 0.44 / 0.66 | 0.54 / 0.74 | 0.72 / 0.85 |
| Resnet50 ConVIRT Softmax | 0.50 / 0.67 | 0.67 / 0.78 | 0.73 / 0.85 |
| **Resnet50 ConVIRT Heuristic Sigmoid** | **0.68 / 0.80** | **0.71 / 0.84** | **0.75 / 0.87** |

> **Káº¿t luáº­n:** MÃ´ hÃ¬nh sá»­ dá»¥ng **Sigmoid Loss vá»›i Heuristic** cho káº¿t quáº£ tá»‘t nháº¥t, vÆ°á»£t trá»™i hÆ¡n Softmax vÃ  ImageNet baseline, Ä‘áº·c biá»‡t trong Ä‘iá»u kiá»‡n Ã­t dá»¯ liá»‡u.

## ğŸ›  CÃ i Ä‘áº·t & Huáº¥n luyá»‡n


### Hyperparameters
* **Batch size:** 64 (pretrain), 32 (downstream).
* **Learning Rate:** 3e-4 (ResNet), 3e-5 (BERT).
* **Temperature ($\tau$):** 0.1 (Softmax), Trainable (Sigmoid).

### YÃªu cáº§u (Requirements)
* Python 3.x
* PyTorch
* Pydicom (xá»­ lÃ½ áº£nh DICOM)
* Transformers (HuggingFace)

## ğŸ”— Tham kháº£o
1.  Ting Chen et al., "A simple framework for contrastive learning of visual representations," arXiv 2020.
2.  Xiaohua Zhai et al., "Sigmoid loss for language image pre-training," ICCV 2023.
3.  Zhang et al., "Contrastive learning of medical visual representations from paired images and text," arXiv 2020.
